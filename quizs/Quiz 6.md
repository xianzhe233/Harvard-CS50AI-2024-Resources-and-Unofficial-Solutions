
Quiz 6
======

Quizzes are _optional, but encouraged_. They are a good way to test your conceptual understanding, before diving into the programming projects. Consider each question below, then reveal the answer. If you didn’t get it right, consider why you may have had that misunderstanding!

Question 1
----------

**Consider the below context-free grammar, where S is the start symbol.**

    S -> NP V
    NP -> N | A NP
    A -> "small" | "white"
    N -> "cats" | "trees"
    V -> "climb" | "run"
    

**Consider also the following four sentences.**

1.  Cats run.
2.  Cats climb trees.
3.  Small cats run.
4.  Small white cats climb.

**Of the four sentences above, which sentences can be derived from the context-free grammar?**

<details><summary>Click here for the answer to Question 1</summary>
Sentence 1, Sentence 3, and Sentence 4.
</details>

Question 2
----------

**Which of the following is _not_ a true statement?**

*   Attention mechanisms can be used to determine which parts of an input sequence are most important to focus on.
*   One-hot representations of words better represent word meaning than distributed representations of words.
*   Transformers can be faster to train than recurrent neural networks because they are more easily parallelized.
*   A Naive Bayes Classifier assumes that the order of words doesn’t matter when determining how they should be classified.

<details><summary>Click here for the answer to Question 2</summary>
One-hot representations of words better represent word meaning than distributed representations of words.
</details>

Question 3
----------

**Why is “smoothing” useful when applying Naive Bayes?**

*   Smoothing allows Naive Bayes to better handle cases where evidence has never appeared for a particular category.
*   Smoothing allows Naive Bayes to better handle cases where there are many categories to classify between, instead of just two.
*   Smoothing allows Naive Bayes to be less “naive” by not assuming that evidence is conditionally independent.
*   Smoothing allows Naive Bayes to turn a conditional probability of evidence given a category into a probability of a category given evidence.

<details><summary>Click here for the answer to Question 3</summary>
Smoothing allows Naive Bayes to better handle cases where evidence has never appeared for a particular category.
</details>

Question 4
----------

**From the phrase “must be the truth”, how many word n-grams of length 2 can be extracted?**

<details><summary>Click here for the answer to Question 4</summary>
3
</details>